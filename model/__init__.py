import torch
import torch.nn as nn

from motion_inbetween.model import transformer


class ContextTransformer(nn.Module):
    def __init__(self, config):
        super(ContextTransformer, self).__init__()
        self.config = config

        self.d_mask = config["d_mask"]
        self.constrained_slices = [
            slice(*i) for i in config["constrained_slices"]
        ]

        self.dropout = config["dropout"]
        self.pre_lnorm = config["pre_lnorm"]
        self.n_layer = config["n_layer"]
        self.geo_embed = config["geo_embed"]

        self.encoder = nn.Sequential(
            nn.Linear(self.config["d_encoder_in"], self.config["d_encoder_h"]),
            nn.GELU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.config["d_encoder_h"], self.config["d_model"]),
            nn.GELU(),
            nn.Dropout(self.dropout)
        )
        self.geo_encoder = nn.Sequential(
            nn.Linear(self.config["d_encoder_in"], self.config["d_model"]),
            nn.GELU(),
            nn.Dropout(self.dropout),
        )

        self.decoder = nn.Sequential(
            nn.Linear(self.config["d_model"], self.config["d_decoder_h"]),
            nn.GELU(),
            nn.Linear(self.config["d_decoder_h"], self.config["d_out"])
        )

        self.rel_pos_layer = nn.Sequential(
            nn.Linear(1, self.config["d_head"]),
            nn.GELU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.config["d_head"], self.config["d_head"]),
            nn.Dropout(self.dropout)
        )

        self.keyframe_pos_layer = nn.Sequential(
            nn.Linear(2, self.config["d_model"]),
            nn.GELU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.config["d_model"], self.config["d_model"]),
            nn.Dropout(self.dropout)
        )

        self.layer_norm = nn.LayerNorm(self.config["d_model"])
        self.att_layers = nn.ModuleList()
        self.pff_layers = nn.ModuleList()

        for i in range(self.n_layer):
            self.att_layers.append(
                transformer.RelMultiHeadedAttention(
                    self.config["n_head"], self.config["d_model"],
                    self.config["d_head"], dropout=self.config["dropout"],
                    pre_lnorm=self.config["pre_lnorm"],
                    bias=self.config["atten_bias"]
                )
            )

            self.pff_layers.append(
                transformer.PositionwiseFeedForward(
                    self.config["d_model"], self.config["d_pff_inner"],
                    dropout=self.config["dropout"],
                    pre_lnorm=self.config["pre_lnorm"]
                )
            )

    def get_rel_pos_emb(self, window_len:int, dtype:torch.dtype, device:torch.device):
        pos_idx = torch.arange(-window_len + 1, window_len,
                               dtype=dtype, device=device)
        pos_idx = pos_idx[None, :, None]        # (1, seq, 1)
        rel_pos_emb = self.rel_pos_layer(pos_idx)
        return rel_pos_emb

    def forward(self, x, keyframe_pos, mask):
        if self.geo_embed=="base":
            x = self.encoder(x)
        elif self.geo_embed=="no":
            seq_part = self.encoder(x[:,12:])
            x = torch.cat([x[:,:12],seq_part],dim=1)
        else:
            seq_part = self.encoder(x[:,12:])
            geo_part = self.geo_encoder(x[:,:12])
            x = torch.cat([geo_part,seq_part],dim=1)
        x = x + self.keyframe_pos_layer(keyframe_pos)

        rel_pos_emb = self.get_rel_pos_emb(x.shape[-2], x.dtype, x.device)

        #for i in range(int(self.n_layer)):
        #    x = self.att_layers[i](x, rel_pos_emb, mask=mask)
        #    x = self.pff_layers[i](x)
        for i,(att_layer,pff_layer) in enumerate(zip(self.att_layers,self.pff_layers)):
            x = att_layer(x, rel_pos_emb, mask=mask)
            x = pff_layer(x)
        #if self.pre_lnorm:
        #    x = self.layer_norm(x)
        x = self.layer_norm(x)
        x = self.decoder(x)

        return x


class DetailTransformer(nn.Module):
    def __init__(self, config):
        super(DetailTransformer, self).__init__()
        self.config = config

        self.d_mask = config["d_mask"]
        self.constrained_slices = [
            slice(*i) for i in config["constrained_slices"]
        ]

        self.dropout = config["dropout"]
        self.pre_lnorm = config["pre_lnorm"]
        self.n_layer = config["n_layer"]

        self.encoder = nn.Sequential(
            nn.Linear(self.config["d_encoder_in"], self.config["d_encoder_h"]),
            nn.PReLU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.config["d_encoder_h"], self.config["d_model"]),
            nn.PReLU(),
            nn.Dropout(self.dropout)
        )

        self.decoder = nn.Sequential(
            nn.Linear(self.config["d_model"], self.config["d_decoder_h"]),
            nn.PReLU(),
            nn.Linear(self.config["d_decoder_h"], self.config["d_out"])
        )

        self.rel_pos_layer = nn.Sequential(
            nn.Linear(1, self.config["d_head"]),
            nn.PReLU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.config["d_head"], self.config["d_head"]),
            nn.Dropout(self.dropout)
        )

        self.layer_norm = nn.LayerNorm(self.config["d_model"])
        self.att_layers = nn.ModuleList()
        self.pff_layers = nn.ModuleList()

        for i in range(self.n_layer):
            self.att_layers.append(
                transformer.RelMultiHeadedAttention(
                    self.config["n_head"], self.config["d_model"],
                    self.config["d_head"], dropout=self.config["dropout"],
                    pre_lnorm=self.config["pre_lnorm"],
                    bias=self.config["atten_bias"]
                )
            )

            self.pff_layers.append(
                transformer.PositionwiseFeedForward(
                    self.config["d_model"], self.config["d_pff_inner"],
                    dropout=self.config["dropout"],
                    pre_lnorm=self.config["pre_lnorm"]
                )
            )

    def get_rel_pos_emb(self, window_len, dtype, device):
        pos_idx = torch.arange(-window_len + 1, window_len,
                               dtype=dtype, device=device)
        pos_idx = pos_idx[None, :, None]        # (1, seq, 1)
        rel_pos_emb = self.rel_pos_layer(pos_idx)
        return rel_pos_emb

    def forward(self, x, mask=None):
        x = self.encoder(x)
        rel_pos_emb = self.get_rel_pos_emb(x.shape[-2], x.dtype, x.device)

        for i in range(self.n_layer):
            x = self.att_layers[i](x, rel_pos_emb, mask=mask)
            x = self.pff_layers[i](x)
        if self.pre_lnorm:
            x = self.layer_norm(x)
        x = self.decoder(x)

        return x


class Discriminator(nn.Module):
    def __init__(self, config):
        super(Discriminator, self).__init__()
        self.config = config

        self.layers = nn.Sequential(
            nn.Conv1d(config["d_in"], config["d_conv1"],
                      kernel_size=config["kernel_size"],
                      stride=1, padding=0),
            nn.ReLU(),
            nn.Conv1d(config["d_conv1"], config["d_conv2"],
                      kernel_size=1, stride=1, padding=0),
            nn.ReLU(),
            nn.Conv1d(config["d_conv2"], 1, kernel_size=1,
                      stride=1, padding=0),
        )

    def forward(self, data):
        data = data.transpose(-1, -2)     # (batch, dim, seq)
        return self.layers(data)          # (batch, 1, seq)
